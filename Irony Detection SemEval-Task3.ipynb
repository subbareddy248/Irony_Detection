{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the librabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\SO186018\\AppData\\Local\\Continuum\\anaconda3_64\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix,classification_report, f1_score\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Reshape, Activation, BatchNormalization\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import LassoLarsCV,RidgeClassifierCV\n",
    "from xgboost.sklearn import XGBRegressor, XGBClassifier\n",
    "import scipy.io as sio\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "from subprocess import check_output\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.neural_network import MLPRegressor,MLPClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "import math\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'skipfeat', 'ngramfeat', 'emojifeat', 'emojisoftfeat', 'hashtagintensityfeat', 'edinberghfeat', 'glovefeat', 'affinfeat', 'binliufeat', 'mpqafeat', 'nrc10efeat', 'nrcemolexfeat', 'nrchashemofeat', 'senti140feat', 'nrchashsentfeat', 'bowfeat', 'tfidffeat', 'structuralfeat', 'sentistrengthfeat', 'trainlabels', 'testlabels', 'trainB', 'testB'])\n"
     ]
    }
   ],
   "source": [
    "dataset = sio.loadmat('./irony.mat')\n",
    "print(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'ngramfeat', 'emojifeat', 'emojisoftfeat', 'edinberghfeat', 'sentistrengthfeat', 'trainlabels'])\n"
     ]
    }
   ],
   "source": [
    "ironyset = sio.loadmat('./irony_only_test.mat')\n",
    "print(ironyset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7201, 300)\n"
     ]
    }
   ],
   "source": [
    "print(ironyset['ngramfeat'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7201, 3208) (7201, 1)\n"
     ]
    }
   ],
   "source": [
    "largefeatures = ['ngramfeat','emojifeat','emojisoftfeat','edinberghfeat', 'sentistrengthfeat', 'trainlabels']\n",
    "X_irony = np.concatenate([ironyset['ngramfeat'],ironyset['emojifeat'],ironyset['emojisoftfeat'],ironyset['edinberghfeat'],ironyset['sentistrengthfeat']],axis=1)\n",
    "\n",
    "Y_irony = ironyset['trainlabels'].T\n",
    "print(X_irony.shape,Y_irony.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'ngramfeat', 'emojifeat', 'emojisoftfeat', 'edinberghfeat', 'sentistrengthfeat', 'trainlabels'])\n",
      "(7072, 140)\n",
      "(7072, 3208) (7072, 1)\n"
     ]
    }
   ],
   "source": [
    "sarcasmset = sio.loadmat('./sarcasm_only_test.mat')\n",
    "print(sarcasmset.keys())\n",
    "print(sarcasmset['sentistrengthfeat'].shape)\n",
    "X_sarcasm = np.concatenate([sarcasmset['ngramfeat'],sarcasmset['emojifeat'],sarcasmset['emojisoftfeat'],sarcasmset['edinberghfeat'],sarcasmset['sentistrengthfeat']],axis=1)\n",
    "\n",
    "Y_sarcasm = sarcasmset['trainlabels'].T\n",
    "print(X_sarcasm.shape,Y_sarcasm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'ngramfeat', 'emojifeat', 'emojisoftfeat', 'edinberghfeat', 'sentistrengthfeat', 'trainlabels'])\n",
      "(8491, 140)\n",
      "(8491, 3208) (8491, 1)\n"
     ]
    }
   ],
   "source": [
    "humorset = sio.loadmat('./humor_only_test.mat')\n",
    "print(humorset.keys())\n",
    "print(humorset['sentistrengthfeat'].shape)\n",
    "X_humor = np.concatenate([humorset['ngramfeat'],humorset['emojifeat'],humorset['emojisoftfeat'],humorset['edinberghfeat'],humorset['sentistrengthfeat']],axis=1)\n",
    "\n",
    "Y_humor = humorset['trainlabels'].T\n",
    "print(X_humor.shape,Y_humor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4618, 4636)\n",
      "(3834, 1) (3834, 1)\n",
      "(784, 1) (784, 1)\n",
      "(3834, 4636)\n",
      "(784, 4636)\n",
      "4\n",
      "(3834, 4636) (3834, 4)\n"
     ]
    }
   ],
   "source": [
    "features = ['ngramfeat','emojifeat','emojisoftfeat','hashtagintensityfeat','edinberghfeat', 'glovefeat', 'affinfeat', 'binliufeat', 'mpqafeat', 'nrc10efeat', 'nrcemolexfeat', 'nrchashemofeat', 'senti140feat', 'nrchashsentfeat', 'structuralfeat', 'sentistrengthfeat']\n",
    "#features1 = ['emojisoftfeat']\n",
    "X = np.concatenate([dataset[feat] for feat in features],axis=1)\n",
    "print(X.shape)\n",
    "Y_train = dataset['trainlabels'].T\n",
    "Y_trainB = dataset['trainB'].T\n",
    "print(Y_train.shape, Y_trainB.shape)\n",
    "no_of_train_sample = Y_train.shape[0]\n",
    "Y_test = dataset['testlabels'].T\n",
    "Y_testB = dataset['testB'].T\n",
    "print(Y_test.shape, Y_testB.shape)\n",
    "no_of_test_samples = Y_test.shape[0]\n",
    "\n",
    "X_train = X[0:no_of_train_sample]\n",
    "print(X_train.shape)\n",
    "X_test = X[no_of_train_sample:no_of_train_sample+no_of_test_samples]\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "no_classes = 4\n",
    "print(no_classes)\n",
    "from keras.utils import to_categorical\n",
    "Y_train_cat = to_categorical(Y_trainB, num_classes = no_classes)\n",
    "print(X_train.shape, Y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-ceb861e081ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_irony\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_irony\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "X_final = np.concatenate([X_train,X_irony],axis=0)\n",
    "Y_final = np.concatenate([Y_train,Y_irony],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble class succsessful build\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "class Ensemble1(object):\n",
    "    def __init__(self, stack, weights='mean', bias=None):\n",
    "        self.stack = stack\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.x = None\n",
    "        \n",
    "    def fit(self, x_train, y_train, lr_moe=0.001, n_epochs_moe=10):\n",
    "        len_s = len(self.stack)\n",
    "        \n",
    "        for b in self.stack:\n",
    "            print(type(b))\n",
    "            #if isinstance(b, KerasClassifier):\n",
    "            #    y_new = y_train_cat\n",
    "            #else:\n",
    "            y_new = y_train\n",
    "            b.fit(x_train, y_new)\n",
    "            print('\\n')\n",
    "        \n",
    "        if self.weights == 'mean':\n",
    "            self.weights = np.full(len_s, 1./len_s)\n",
    "        elif self.weights == 'moe':\n",
    "            print('Train moe algorithm on {}\\n'.format(x_train.shape[0]))\n",
    "            \n",
    "            self.x = np.random.uniform(low=0, high=1, size=len_s)\n",
    "            self.bias = np.random.uniform(low=0, high=5, size=len_s)\n",
    "            \n",
    "            for t in range(1, n_epochs_moe + 1):                \n",
    "                y_predict = np.vstack(self.stack_predict(x_train))\n",
    "                prob = softmax(self.x)        \n",
    "                \n",
    "                for k in range(x_train.shape[0]):\n",
    "                    dG_dx = []\n",
    "                    dG_db = []\n",
    "                    \n",
    "                    for i in range(len_s):                     \n",
    "                        dG_dx.append(0.5 * prob[i] * (1 - prob[i]) * (y_train - y_predict[i][k] + self.bias[i])**2)\n",
    "                        dG_db.append(prob[i] * (y_train - y_predict[i][k] + self.bias[i]))\n",
    "                             \n",
    "                        self.x[i] -= lr_moe * dG_dx[i][k]\n",
    "                        self.bias[i] -= lr_moe * dG_db[i][k]\n",
    "                    \n",
    "                print('Epoch {}; weights {}; alpha {}'.format(t, softmax(self.x), self.bias))\n",
    "                \n",
    "            self.weights = softmax(self.x)\n",
    "        \n",
    "    def stack_predict(self, x_valid):\n",
    "        b = [] \n",
    "        for b_ in self.stack:\n",
    "            b.append(b_.predict(x_valid))\n",
    "            \n",
    "        return b\n",
    "    \n",
    "    def predict(self, x_valid):              \n",
    "        predict = np.average(self.stack_predict(x_valid), axis=0, weights=self.weights)\n",
    "        \n",
    "        if self.bias != None:\n",
    "            predict += np.average(self.bias, weights=self.weights, axis=0)\n",
    "        return predict\n",
    "  \n",
    "\n",
    "print('Ensemble class succsessful build')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_model():\n",
    "    GBoost=GradientBoostingClassifier(n_estimators=400,learning_rate=0.1,random_state=100,max_features=4 )\n",
    "    return GBoost\n",
    "\n",
    "model_gb = create_ml_model()\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(max_depth=25, \n",
    "                        learning_rate=0.1, \n",
    "                        n_estimators=300)\n",
    "\n",
    "LightGB = lgb.LGBMClassifier(num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "lasso = LassoLarsCV(normalize=True)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=6, \n",
    "    min_samples_split=10, \n",
    "    class_weight=\"balanced\",\n",
    "    random_state=100\n",
    "    )\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "ada = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=4),n_estimators=400, learning_rate=0.1,random_state=100)\n",
    "\n",
    "clf = svm.SVC(kernel=\"linear\", C=0.35, probability=True, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "\n",
      "<class 'brew.stacking.stacker.EnsembleStackClassifier'>\n",
      "\n",
      "\n",
      "Train moe algorithm on 3834\n",
      "\n",
      "Epoch 1; weights [ 0.5668652  0.4331348]; alpha [ 0.14903159  0.08267374]\n",
      "Epoch 2; weights [ 0.56651143  0.43348857]; alpha [ 0.01694853  0.01570381]\n",
      "Epoch 3; weights [ 0.56651215  0.43348785]; alpha [ 0.00193008  0.00297888]\n",
      "Epoch 4; weights [ 0.56651235  0.43348765]; alpha [ 0.00021979  0.00056507]\n",
      "Epoch 5; weights [ 0.56651236  0.43348764]; alpha [  2.50298480e-05   1.07189524e-04]\n",
      "Epoch 6; weights [ 0.56651236  0.43348764]; alpha [  2.85035965e-06   2.03330283e-05]\n",
      "Epoch 7; weights [ 0.56651236  0.43348764]; alpha [  3.24594465e-07   3.85701909e-06]\n",
      "Epoch 8; weights [ 0.56651236  0.43348764]; alpha [  3.69643062e-08   7.31646859e-07]\n",
      "Epoch 9; weights [ 0.56651236  0.43348764]; alpha [  4.20943694e-09   1.38787782e-07]\n",
      "Epoch 10; weights [ 0.56651236  0.43348764]; alpha [  4.79364045e-10   2.63269750e-08]\n"
     ]
    }
   ],
   "source": [
    "model = Ensemble1(stack=[mlp,clf1], weights='moe')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591304347826087 0.6559485530546624 0.6219512195121951\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.70      0.73       473\n",
      "          1       0.59      0.66      0.62       311\n",
      "\n",
      "avg / total       0.69      0.68      0.69       784\n",
      "\n",
      "[[332 141]\n",
      " [107 204]]\n",
      "0.591304347826087 0.6559485530546624 0.6219512195121951\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.70      0.73       473\n",
      "          1       0.59      0.66      0.62       311\n",
      "\n",
      "avg / total       0.69      0.68      0.69       784\n",
      "\n",
      "[[332 141]\n",
      " [107 204]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_model = [int(i) for i in model.predict(X_test)]\n",
    "p, r, f = evaluate.precision_recall_fscore(Y_true, Y_pred_model, beta=1, labels=[0,1],pos_label=1)\n",
    "print(p,r,f)\n",
    "print(classification_report(Y_true,Y_pred_model))\n",
    "print(confusion_matrix(Y_true,Y_pred_model))\n",
    "Y_pred_model = model.predict(X_test)\n",
    "Y_pred_model = [int(i) for i in Y_pred_model]\n",
    "p, r, f = evaluate.precision_recall_fscore(Y_true, Y_pred_model, beta=1, labels=[0,1],pos_label=1)\n",
    "print(p,r,f)\n",
    "\n",
    "print(classification_report(Y_true,Y_pred_model))\n",
    "print(confusion_matrix(Y_true,Y_pred_model))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "filename = 'irony_taskA_model_transfer.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'irony_taskA_model_transfer.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#result = loaded_model.score(X_humor, Y_humor)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       1.00      0.68      0.81      7201\n",
      "\n",
      "avg / total       1.00      0.68      0.81      7201\n",
      "\n",
      "[[   0    0]\n",
      " [2290 4911]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       1.00      0.68      0.81      7072\n",
      "\n",
      "avg / total       1.00      0.68      0.81      7072\n",
      "\n",
      "[[   0    0]\n",
      " [2298 4774]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       1.00      0.80      0.89      8491\n",
      "\n",
      "avg / total       1.00      0.80      0.89      8491\n",
      "\n",
      "[[   0    0]\n",
      " [1660 6831]]\n"
     ]
    }
   ],
   "source": [
    "Y_true_irony = [int(i) for i in loaded_model.predict(X_irony)]\n",
    "print(classification_report(Y_irony,Y_true_irony))\n",
    "print(confusion_matrix(Y_irony,Y_true_irony))\n",
    "\n",
    "Y_true_sarcasm = [int(i) for i in loaded_model.predict(X_sarcasm)]\n",
    "print(classification_report(Y_sarcasm,Y_true_sarcasm))\n",
    "print(confusion_matrix(Y_sarcasm,Y_true_sarcasm))\n",
    "\n",
    "Y_true_humor = [int(i) for i in loaded_model.predict(X_humor)]\n",
    "print(classification_report(Y_humor,Y_true_humor))\n",
    "print(confusion_matrix(Y_humor,Y_true_humor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   6   10   12 ..., 7180 7186 7189]\n"
     ]
    }
   ],
   "source": [
    "print(np.where(np.array(Y_true_irony)==0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       1.00      0.68      0.81      7201\n",
      "\n",
      "avg / total       1.00      0.68      0.81      7201\n",
      "\n",
      "[[   0    0]\n",
      " [2290 4911]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         0\n",
      "        1.0       1.00      0.68      0.81      7072\n",
      "\n",
      "avg / total       1.00      0.68      0.81      7072\n",
      "\n",
      "[[   0    0]\n",
      " [2298 4774]]\n"
     ]
    }
   ],
   "source": [
    "Y_true_irony = [int(i) for i in model.predict(X_irony)]\n",
    "print(classification_report(Y_irony,Y_true_irony))\n",
    "print(confusion_matrix(Y_irony,Y_true_irony))\n",
    "\n",
    "Y_true_sarcasm = [int(i) for i in model.predict(X_sarcasm)]\n",
    "print(classification_report(Y_sarcasm,Y_true_sarcasm))\n",
    "print(confusion_matrix(Y_sarcasm,Y_true_sarcasm))\n",
    "#Y_true = [int(i) for i in Y_testB]\n",
    "#p, r, f = evaluate.precision_recall_fscore(Y_true, Y_pred, beta=1, labels=[0,1,2,3])\n",
    "#print(p,r,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.88      0.72       473\n",
      "          1       0.10      0.04      0.06       164\n",
      "          2       0.14      0.06      0.08        85\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.41      0.55      0.46       784\n",
      "\n",
      "0.21424889543446246 0.24577830539218215 0.21678018162393162\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_true,Y_pred))\n",
    "Y_true = [int(i) for i in Y_testB]\n",
    "p, r, f = evaluate.precision_recall_fscore(Y_true, Y_pred, beta=1, labels=[0,1,2,3])\n",
    "print(p,r,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second feature matrix\n",
    "X_train2 = pd.DataFrame( {'moe': model.predict(X_train),\n",
    "     #'GBT': gbr.predict(X_train),\n",
    "    #'GBTT':GBoost.predict(X_train),\n",
    "    # 'LGBT': LightGB.predict(X_train),\n",
    "    #'ET': extreg.predict(X_train),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 1 0 1 0 0] [0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "0.5497737556561086 0.7813504823151125 0.6454183266932271\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LogisticRegression()\n",
    "reg1 = linear_model.Ridge()\n",
    "reg.fit(X_train2, Y_train)\n",
    "\n",
    "# prediction using the test set\n",
    "X_test2 = pd.DataFrame( {'moe': model.predict(X_test),\n",
    "     #'GBT': gbr.predict(X_test),\n",
    "    #'GBTT': GBoost.predict(X_test),\n",
    "     #'LGBT': LightGB.predict(X_test),\n",
    "    #'ET': extreg.predict(X_test),\n",
    "    })\n",
    "# Don't forget to convert the prediction back to non-log scale\n",
    "Y_test_pred = reg.predict(X_test2)\n",
    "Y_test_reg_new = [int(i) for i in Y_test]\n",
    "#y_test_pred = [round(float(i),3) for i in Y_test_pred]\n",
    "print(Y_test_pred[0:10],Y_test_reg_new[0:10])\n",
    "p, r, f = evaluate.precision_recall_fscore(Y_true, Y_test_pred, beta=1, labels=[0,1], pos_label=1)\n",
    "print(p,r,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brew\n",
    "from brew.base import Ensemble\n",
    "from brew.combination.combiner import Combiner\n",
    "from brew.stacking.stacker import EnsembleStack\n",
    "from brew.stacking.stacker import EnsembleStackClassifier\n",
    " \n",
    "layer_1 = [#svm.SVC(probability=True),\n",
    "           XGBClassifier(max_depth=6,n_estimators=100),\n",
    "           RandomForestClassifier(n_estimators=100), \n",
    "           MLPClassifier(),\n",
    "           #GradientBoostingClassifier()\n",
    "          ]\n",
    " \n",
    "layer_2 = [linear_model.LogisticRegression(max_iter=500), MLPClassifier()]\n",
    " \n",
    "stack1 = EnsembleStack(cv=0) # number of folds per layer\n",
    "stack1.add_layer(Ensemble(layer_1))\n",
    "stack1.add_layer(Ensemble(layer_2))\n",
    "#stack1.add_layer(Ensemble(model))\n",
    "clf1 = EnsembleStackClassifier(stack1, Combiner('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7a520eb0b6db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os_features' is not defined"
     ]
    }
   ],
   "source": [
    "clf1.fit(os_features, os_labels)\n",
    "clf1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.79      0.74       473\n",
      "          1       0.50      0.70      0.58       164\n",
      "          2       0.69      0.21      0.32        85\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.61      0.64      0.61       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_testB, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99770174,  0.00229826],\n",
       "       [ 0.77650473,  0.22349527],\n",
       "       [ 0.6277022 ,  0.3722978 ],\n",
       "       ..., \n",
       "       [ 0.99340277,  0.00659723],\n",
       "       [ 0.05285746,  0.94714254],\n",
       "       [ 0.99003292,  0.00996708]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.66      0.70       473\n",
      "          1       0.55      0.65      0.60       311\n",
      "\n",
      "avg / total       0.67      0.65      0.66       784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = [int(i) for i in Y_test]\n",
    "Y_trueB = [int(i) for i in Y_testB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f = evaluate.precision_recall_fscore(Y_true, clf.predict(X_test), beta=1, labels=[0,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5537190082644629 0.6463022508038585 0.5964391691394658\n"
     ]
    }
   ],
   "source": [
    "print(p,r,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtagintensityfeat\n",
      "0.0 0.0 0.0\n",
      "structuralfeat\n",
      "0.47072072072072074 0.6720257234726688 0.5536423841059602\n"
     ]
    }
   ],
   "source": [
    "sm = SMOTE(ratio='auto',kind='regular')\n",
    "for i in features3:\n",
    "    print (i)\n",
    "    XX = dataset[i][0:no_of_train_sample]\n",
    "    XT = dataset[i][no_of_train_sample:no_of_train_sample+no_of_test_samples] \n",
    "    #os_features,os_labels=sm.fit_sample(XX,Y_train)\n",
    "    clf1.fit(XX, Y_train)\n",
    "    p, r, f = evaluate.precision_recall_fscore(Y_true, clf1.predict(XT), beta=1, labels=[0,1], pos_label=1)\n",
    "    print(p,r,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4618, 3208)\n",
      "(3834, 1) (3834, 1)\n",
      "(784, 1) (784, 1)\n",
      "(3834, 3208)\n",
      "(784, 3208)\n",
      "4\n",
      "(3834, 3208) (3834, 4)\n"
     ]
    }
   ],
   "source": [
    "features1 = ['skipfeat','emojifeat','ngramfeat','emojisoftfeat','edinberghfeat', 'glovefeat','bowfeat','tfidffeat', 'binliufeat', 'mpqafeat','nrcemolexfeat', 'nrchashemofeat', 'senti140feat', 'sentistrengthfeat']\n",
    "features2 = ['emojifeat','ngramfeat','emojisoftfeat','edinberghfeat','sentistrengthfeat']\n",
    "#features3 = ['hashtagintensityfeat','structuralfeat']\n",
    "\n",
    "X = np.concatenate([dataset[feat] for feat in features2],axis=1)\n",
    "print(X.shape)\n",
    "Y_train = dataset['trainlabels'].T\n",
    "Y_trainB = dataset['trainB'].T\n",
    "print(Y_train.shape, Y_trainB.shape)\n",
    "no_of_train_sample = Y_train.shape[0]\n",
    "Y_test = dataset['testlabels'].T\n",
    "Y_testB = dataset['testB'].T\n",
    "print(Y_test.shape, Y_testB.shape)\n",
    "no_of_test_samples = Y_test.shape[0]\n",
    "\n",
    "X_train = X[0:no_of_train_sample]\n",
    "print(X_train.shape)\n",
    "X_test = X[no_of_train_sample:no_of_train_sample+no_of_test_samples]\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "no_classes = 4\n",
    "print(no_classes)\n",
    "from keras.utils import to_categorical\n",
    "Y_train_cat = to_categorical(Y_trainB, num_classes = no_classes)\n",
    "print(X_train.shape, Y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "sm = SMOTE(ratio='auto',kind='regular')\n",
    "os_features,os_labels=sm.fit_sample(X_train,Y_train)\n",
    "#us_features,us_labels = NearMiss(ratio='auto').fit_sample(X_train,Y_trainB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5680628272251309 0.6977491961414791 0.6262626262626264\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.65      0.70       473\n",
      "          1       0.57      0.70      0.63       311\n",
      "\n",
      "avg / total       0.69      0.67      0.67       784\n",
      "\n",
      "[[308 165]\n",
      " [ 94 217]]\n"
     ]
    }
   ],
   "source": [
    "Y_train2 = [int(i) for i in Y_trainB]\n",
    "Y_train2 = np.array(Y_train2)\n",
    "clf1.fit(X_train, Y_train)\n",
    "p, r, f = evaluate.precision_recall_fscore(Y_true, clf1.predict(X_test), beta=1, labels=[0,1],pos_label=1)\n",
    "print(p,r,f)\n",
    "print(classification_report(Y_true,clf1.predict(X_test)))\n",
    "print(confusion_matrix(Y_true,clf1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f = evaluate.precision_recall_fscore(Y_trueB, clf1.predict(X_test), beta=1, labels=[0,1,2,3])\n",
    "print(p,r,f)\n",
    "print(classification_report(Y_trueB,clf1.predict(X_test)))\n",
    "print(confusion_matrix(Y_trueB,clf1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
